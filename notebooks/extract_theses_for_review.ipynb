{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Extraction for Manual Review\n",
    "\n",
    "This notebook:\n",
    "1. Loads PDF files from `data/latest_thesis_decks`\n",
    "2. Uses GPT-4 to extract all thesis statements\n",
    "3. Numbers each thesis uniquely\n",
    "4. Generates 2-3 sentence descriptions\n",
    "5. Identifies which theses support other theses\n",
    "6. Exports to CSV for easy human review\n",
    "\n",
    "**Output:** A clean CSV file with all theses ready for manual review and refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports and configuration loaded\n",
      "  Data folder: C:\\Users\\ChetKumar\\Projects\\thesis-bot\\notebooks\\..\\data\\latest_thesis_decks\n",
      "  Output folder: C:\\Users\\ChetKumar\\Projects\\thesis-bot\\notebooks\\..\\data\\analysis\n",
      "  OpenAI client: âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re  # For JSON extraction\n",
    "import time  # For waiting on assistant runs\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\").resolve()\n",
    "load_dotenv(env_path)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
    "\n",
    "DATA_FOLDER = Path(\"../data/latest_thesis_decks\")\n",
    "OUTPUT_FOLDER = Path(\"../data/analysis\")\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"âœ“ Imports and configuration loaded\")\n",
    "print(f\"  Data folder: {DATA_FOLDER.absolute()}\")\n",
    "print(f\"  Output folder: {OUTPUT_FOLDER.absolute()}\")\n",
    "print(f\"  OpenAI client: {'âœ“' if openai_client else 'âœ—'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: AI 20250923.pdf\n",
      "  âœ“ Extracted 45,833 characters\n",
      "Extracting text from: Argonautic BioTech Presentation 20250412.pdf\n",
      "  âœ“ Extracted 28,462 characters\n",
      "Extracting text from: Argonautic Consolidated Thesis Presentation 20250818.pdf\n",
      "  âœ“ Extracted 47,895 characters\n",
      "Extracting text from: Argonautic Construction Technology 20250412.pdf\n",
      "  âœ“ Extracted 35,062 characters\n",
      "\n",
      "âœ“ Loaded 4 PDF file(s)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare PDF files for upload\n",
    "# (No text extraction needed - we'll upload PDFs directly to OpenAI)\n",
    "\n",
    "pdf_files = list(DATA_FOLDER.glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"âš  No PDF files found in {DATA_FOLDER}\")\n",
    "    pdf_files = []\n",
    "\n",
    "print(f\"âœ“ Found {len(pdf_files)} PDF file(s) ready for upload\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"  - {pdf_file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting theses from: AI 20250923.pdf\n",
      "  âœ“ Found 11 theses, 0 support relationships\n",
      "\n",
      "Extracting theses from: Argonautic BioTech Presentation 20250412.pdf\n",
      "  âœ“ Found 9 theses, 0 support relationships\n",
      "\n",
      "Extracting theses from: Argonautic Consolidated Thesis Presentation 20250818.pdf\n",
      "  âœ“ Found 18 theses, 0 support relationships\n",
      "\n",
      "Extracting theses from: Argonautic Construction Technology 20250412.pdf\n",
      "  âœ“ Found 7 theses, 0 support relationships\n",
      "\n",
      "âœ“ Extraction complete for 4 document(s)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract theses using OpenAI Assistants API (handles PDFs directly)\n",
    "\n",
    "def extract_theses_from_pdf(pdf_path: Path, filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload PDF directly to OpenAI and extract theses using Assistants API.\n",
    "    This processes the entire PDF without truncation.\n",
    "    \"\"\"\n",
    "    if not openai_client:\n",
    "        print(\"âš  OpenAI client not configured.\")\n",
    "        return {\"theses\": [], \"thesis_supports\": []}\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Upload the PDF file\n",
    "        print(f\"  Uploading {filename} to OpenAI...\")\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            uploaded_file = openai_client.files.create(\n",
    "                file=file,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "        print(f\"  âœ“ File uploaded (ID: {uploaded_file.id})\")\n",
    "        \n",
    "        # Step 2: Create an assistant with instructions\n",
    "        assistant = openai_client.beta.assistants.create(\n",
    "            name=\"Thesis Extractor\",\n",
    "            instructions=\"\"\"You are an expert at analyzing documents and extracting thesis statements. \n",
    "\n",
    "Extract ALL thesis statements/arguments from the document. A thesis is a claim, argument, or proposition that the document is making.\n",
    "\n",
    "For each thesis, provide:\n",
    "1. The exact thesis statement (the claim being made)\n",
    "2. A brief 2-3 sentence description that accurately describes what the thesis is about\n",
    "\n",
    "Also identify relationships: if one thesis supports or provides evidence for another thesis, note that.\n",
    "\n",
    "IMPORTANT:\n",
    "- Extract ALL theses, even if they seem similar (we will deduplicate later)\n",
    "- Be thorough - don't miss any arguments or claims\n",
    "- Each thesis should be a distinct statement\n",
    "- If theses are redundant or very similar, still list them (we'll handle deduplication)\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "{\n",
    "    \"theses\": [\n",
    "        {\n",
    "            \"thesis\": \"The exact thesis statement\",\n",
    "            \"description\": \"2-3 sentences accurately describing what this thesis is about\"\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    \"thesis_supports\": [\n",
    "        {\n",
    "            \"source_thesis\": \"The supporting thesis statement\",\n",
    "            \"target_thesis\": \"The thesis statement it supports\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "Return ONLY valid JSON, no other text.\"\"\",\n",
    "            model=\"gpt-4-turbo-preview\",  # or \"gpt-4o\" for faster/cheaper\n",
    "            tools=[{\"type\": \"code_interpreter\"}],  # Helps with document analysis\n",
    "        )\n",
    "        \n",
    "        # Step 3: Create a thread and add the file\n",
    "        thread = openai_client.beta.threads.create()\n",
    "        \n",
    "        message = openai_client.beta.threads.messages.create(\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=\"Extract all thesis statements from this document. Return the JSON structure as specified in your instructions.\",\n",
    "            file_ids=[uploaded_file.id]\n",
    "        )\n",
    "        \n",
    "        # Step 4: Run the assistant\n",
    "        print(f\"  Analyzing document (this may take a minute)...\")\n",
    "        run = openai_client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id\n",
    "        )\n",
    "        \n",
    "        # Step 5: Wait for completion\n",
    "        max_wait = 300  # 5 minutes max\n",
    "        wait_time = 0\n",
    "        while run.status in [\"queued\", \"in_progress\"]:\n",
    "            time.sleep(2)\n",
    "            wait_time += 2\n",
    "            if wait_time > max_wait:\n",
    "                print(f\"  âš  Timeout waiting for analysis\")\n",
    "                break\n",
    "            run = openai_client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "            if wait_time % 10 == 0:\n",
    "                print(f\"    Still processing... ({wait_time}s)\")\n",
    "        \n",
    "        if run.status == \"completed\":\n",
    "            # Step 6: Get the response\n",
    "            messages = openai_client.beta.threads.messages.list(\n",
    "                thread_id=thread.id\n",
    "            )\n",
    "            \n",
    "            # Extract JSON from the response\n",
    "            result_text = messages.data[0].content[0].text.value\n",
    "            \n",
    "            # Try to extract JSON\n",
    "            if \"```json\" in result_text:\n",
    "                result_text = result_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in result_text:\n",
    "                result_text = result_text.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            # Find JSON object in the text\n",
    "            json_match = re.search(r'\\{.*\\}', result_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                result_text = json_match.group(0)\n",
    "            \n",
    "            result = json.loads(result_text.strip())\n",
    "            result[\"filename\"] = filename\n",
    "            \n",
    "            # Cleanup: delete the file and assistant\n",
    "            try:\n",
    "                openai_client.files.delete(uploaded_file.id)\n",
    "                openai_client.beta.assistants.delete(assistant.id)\n",
    "            except:\n",
    "                pass  # Ignore cleanup errors\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            print(f\"  âœ— Run failed with status: {run.status}\")\n",
    "            if run.status == \"failed\":\n",
    "                print(f\"    Error: {run.last_error}\")\n",
    "            # Cleanup on failure\n",
    "            try:\n",
    "                openai_client.files.delete(uploaded_file.id)\n",
    "                openai_client.beta.assistants.delete(assistant.id)\n",
    "            except:\n",
    "                pass\n",
    "            return {\"theses\": [], \"thesis_supports\": [], \"filename\": filename}\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"theses\": [], \"thesis_supports\": [], \"filename\": filename}\n",
    "\n",
    "# Extract theses from all PDFs using direct file upload\n",
    "all_extractions = {}YY\n",
    "for pdf_file in pdf_files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Extracting theses from: {pdf_file.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    extraction = extract_theses_from_pdf(pdf_file, pdf_file.name)\n",
    "    all_extractions[pdf_file.name] = extraction\n",
    "    print(f\"  âœ“ Found {len(extraction.get('theses', []))} theses, {len(extraction.get('thesis_supports', []))} support relationships\")\n",
    "\n",
    "print(f\"\\nâœ“ Extraction complete for {len(all_extractions)} document(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Deduplicate and number theses\n",
    "\n",
    "def deduplicate_theses(all_extractions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine theses from all documents, deduplicate similar ones, and assign unique numbers.\n",
    "    \"\"\"\n",
    "    all_theses = []\n",
    "    thesis_to_number = {}\n",
    "    thesis_number = 1\n",
    "    \n",
    "    # First pass: collect all theses with their source files\n",
    "    for filename, extraction in all_extractions.items():\n",
    "        for thesis_data in extraction.get(\"theses\", []):\n",
    "            thesis = thesis_data.get(\"thesis\", \"\").strip()\n",
    "            description = thesis_data.get(\"description\", \"\").strip()\n",
    "            \n",
    "            if thesis:  # Only add non-empty theses\n",
    "                all_theses.append({\n",
    "                    \"thesis\": thesis,\n",
    "                    \"description\": description,\n",
    "                    \"source_file\": filename\n",
    "                })\n",
    "    \n",
    "    # Second pass: deduplicate similar theses\n",
    "    # Group theses by similarity (simple approach: exact match or very similar)\n",
    "    unique_theses = []\n",
    "    seen_theses = set()\n",
    "    \n",
    "    for thesis_data in all_theses:\n",
    "        thesis = thesis_data[\"thesis\"]\n",
    "        thesis_lower = thesis.lower().strip()\n",
    "        \n",
    "        # Check if we've seen a very similar thesis\n",
    "        is_duplicate = False\n",
    "        for seen in seen_theses:\n",
    "            # Simple similarity check: if one is contained in the other or very similar\n",
    "            if (thesis_lower in seen.lower() or seen.lower() in thesis_lower) and abs(len(thesis_lower) - len(seen)) < 20:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            seen_theses.add(thesis)\n",
    "            unique_theses.append(thesis_data)\n",
    "            thesis_to_number[thesis] = thesis_number\n",
    "            thesis_number += 1\n",
    "    \n",
    "    # Build thesis supports relationships with numbers\n",
    "    thesis_supports = []\n",
    "    for filename, extraction in all_extractions.items():\n",
    "        for rel in extraction.get(\"thesis_supports\", []):\n",
    "            source = rel.get(\"source_thesis\", \"\").strip()\n",
    "            target = rel.get(\"target_thesis\", \"\").strip()\n",
    "            \n",
    "            # Find the unique thesis numbers\n",
    "            source_num = None\n",
    "            target_num = None\n",
    "            \n",
    "            for thesis, num in thesis_to_number.items():\n",
    "                if source and (source in thesis or thesis in source):\n",
    "                    source_num = num\n",
    "                if target and (target in thesis or thesis in target):\n",
    "                    target_num = num\n",
    "            \n",
    "            if source_num and target_num:\n",
    "                thesis_supports.append({\n",
    "                    \"source_thesis_number\": source_num,\n",
    "                    \"target_thesis_number\": target_num,\n",
    "                    \"source_thesis\": source,\n",
    "                    \"target_thesis\": target\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"theses\": unique_theses,\n",
    "        \"thesis_to_number\": thesis_to_number,\n",
    "        \"thesis_supports\": thesis_supports\n",
    "    }\n",
    "\n",
    "# Deduplicate and number\n",
    "print(\"Deduplicating theses...\")\n",
    "deduplicated = deduplicate_theses(all_extractions)\n",
    "print(f\"âœ“ Found {len(deduplicated['theses'])} unique theses (from {sum(len(e.get('theses', [])) for e in all_extractions.values())} total)\")\n",
    "print(f\"âœ“ Found {len(deduplicated['thesis_supports'])} support relationships\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create review-friendly CSV\n",
    "\n",
    "def create_review_csv(deduplicated: Dict[str, Any], output_file: Path):\n",
    "    \"\"\"\n",
    "    Create a CSV file optimized for human review.\n",
    "    \"\"\"\n",
    "    thesis_to_number = deduplicated[\"thesis_to_number\"]\n",
    "    thesis_supports = deduplicated[\"thesis_supports\"]\n",
    "    \n",
    "    # Build a map of which theses each thesis supports\n",
    "    supports_map = {}\n",
    "    for rel in thesis_supports:\n",
    "        source_num = rel[\"source_thesis_number\"]\n",
    "        target_num = rel[\"target_thesis_number\"]\n",
    "        \n",
    "        if source_num not in supports_map:\n",
    "            supports_map[source_num] = []\n",
    "        supports_map[source_num].append(target_num)\n",
    "    \n",
    "    # Create rows for CSV\n",
    "    rows = []\n",
    "    for thesis_data in deduplicated[\"theses\"]:\n",
    "        thesis = thesis_data[\"thesis\"]\n",
    "        thesis_num = thesis_to_number[thesis]\n",
    "        \n",
    "        # Get list of thesis numbers this thesis supports\n",
    "        supports_list = sorted(supports_map.get(thesis_num, []))\n",
    "        supports_str = \", \".join(map(str, supports_list)) if supports_list else \"\"\n",
    "        \n",
    "        rows.append({\n",
    "            \"Thesis Number\": thesis_num,\n",
    "            \"Thesis Statement\": thesis,\n",
    "            \"Description\": thesis_data[\"description\"],\n",
    "            \"Supports Thesis Numbers\": supports_str,\n",
    "            \"Source File\": thesis_data[\"source_file\"]\n",
    "        })\n",
    "    \n",
    "    # Sort by thesis number\n",
    "    rows.sort(key=lambda x: x[\"Thesis Number\"])\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the review CSV\n",
    "output_file = OUTPUT_FOLDER / \"theses_for_review.csv\"\n",
    "df = create_review_csv(deduplicated, output_file)\n",
    "\n",
    "print(f\"\\nâœ“ Created review CSV: {output_file}\")\n",
    "print(f\"\\nCSV Summary:\")\n",
    "print(f\"  Total theses: {len(df)}\")\n",
    "print(f\"  Theses with support relationships: {len(df[df['Supports Thesis Numbers'] != ''])}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10).to_string())\n",
    "print(f\"\\nðŸ“ Next steps:\")\n",
    "print(f\"  1. Open {output_file} in Excel or a text editor\")\n",
    "print(f\"  2. Review each thesis statement and description\")\n",
    "print(f\"  3. Ensure descriptions are 2-3 sentences and accurate\")\n",
    "print(f\"  4. Verify and update 'Supports Thesis Numbers' column\")\n",
    "print(f\"  5. Save as 'theses_reviewed.csv' when done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
