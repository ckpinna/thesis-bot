{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Extraction for Manual Review\n",
    "\n",
    "This notebook:\n",
    "1. Loads PDF files from `data/latest_thesis_decks`\n",
    "2. Uses GPT-4 to extract all thesis statements\n",
    "3. Numbers each thesis uniquely\n",
    "4. Generates 2-3 sentence descriptions\n",
    "5. Identifies which theses support other theses\n",
    "6. Exports to CSV for easy human review\n",
    "\n",
    "**Output:** A clean CSV file with all theses ready for manual review and refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Imports and configuration loaded\n",
      "  Data folder: C:\\Users\\ChetKumar\\Projects\\thesis-bot\\notebooks\\..\\data\\latest_thesis_decks\n",
      "  Output folder: C:\\Users\\ChetKumar\\Projects\\thesis-bot\\notebooks\\..\\data\\analysis\n",
      "  OpenAI client: âœ“\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re  # For JSON extraction\n",
    "import time  # For waiting on assistant runs\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"../.env\").resolve()\n",
    "load_dotenv(env_path)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None\n",
    "\n",
    "DATA_FOLDER = Path(\"../data/latest_thesis_decks\")\n",
    "OUTPUT_FOLDER = Path(\"../data/analysis\")\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"âœ“ Imports and configuration loaded\")\n",
    "print(f\"  Data folder: {DATA_FOLDER.absolute()}\")\n",
    "print(f\"  Output folder: {OUTPUT_FOLDER.absolute()}\")\n",
    "print(f\"  OpenAI client: {'âœ“' if openai_client else 'âœ—'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found 4 PDF file(s) ready for upload\n",
      "  - AI 20250923.pdf\n",
      "  - Argonautic BioTech Presentation 20250412.pdf\n",
      "  - Argonautic Consolidated Thesis Presentation 20250818.pdf\n",
      "  - Argonautic Construction Technology 20250412.pdf\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load PDFs and extract text\n",
    "# We extract text ourselves to ensure the full document is analyzed\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def load_all_pdfs(data_folder: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load all PDF files from the data folder and extract their text.\"\"\"\n",
    "    pdf_texts = {}\n",
    "    pdf_files = list(data_folder.glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"âš  No PDF files found in {data_folder}\")\n",
    "        return pdf_texts\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Extracting text from: {pdf_file.name}\")\n",
    "        try:\n",
    "            text = extract_text_from_pdf(pdf_file)\n",
    "            pdf_texts[pdf_file.name] = text\n",
    "            print(f\"  âœ“ Extracted {len(text):,} characters\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error extracting {pdf_file.name}: {e}\")\n",
    "    \n",
    "    return pdf_texts\n",
    "\n",
    "# Load all PDFs\n",
    "pdf_texts = load_all_pdfs(DATA_FOLDER)\n",
    "print(f\"\\nâœ“ Loaded {len(pdf_texts)} PDF file(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Extracting theses from: AI 20250923.pdf\n",
      "============================================================\n",
      "  Analyzing document (45,833 characters)...\n",
      "  âœ“ Found 10 theses, 0 support relationships\n",
      "\n",
      "============================================================\n",
      "Extracting theses from: Argonautic BioTech Presentation 20250412.pdf\n",
      "============================================================\n",
      "  Analyzing document (28,462 characters)...\n",
      "  âœ“ Found 6 theses, 3 support relationships\n",
      "\n",
      "============================================================\n",
      "Extracting theses from: Argonautic Consolidated Thesis Presentation 20250818.pdf\n",
      "============================================================\n",
      "  Analyzing document (47,895 characters)...\n",
      "  âœ“ Found 9 theses, 5 support relationships\n",
      "\n",
      "============================================================\n",
      "Extracting theses from: Argonautic Construction Technology 20250412.pdf\n",
      "============================================================\n",
      "  Analyzing document (35,062 characters)...\n",
      "  âœ“ Found 13 theses, 3 support relationships\n",
      "\n",
      "âœ“ Extraction complete for 4 document(s)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract theses using GPT-4 Turbo with full document text\n",
    "\n",
    "def extract_theses_from_text(text: str, filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Use GPT-4 Turbo to extract theses from the full document text.\n",
    "    GPT-4 Turbo has 128K token context, so we can send the entire document.\n",
    "    \"\"\"\n",
    "    if not openai_client:\n",
    "        print(\"âš  OpenAI client not configured.\")\n",
    "        return {\"theses\": [], \"thesis_supports\": []}\n",
    "    \n",
    "    # GPT-4 Turbo: 128K tokens â‰ˆ 500K characters\n",
    "    # Reserve ~10K tokens for prompt/system, so ~470K characters for text\n",
    "    max_chars = 470000\n",
    "    \n",
    "    # If document is longer, we'll process in chunks\n",
    "    if len(text) > max_chars:\n",
    "        print(f\"  âš  Document is very long ({len(text):,} chars). Processing in chunks...\")\n",
    "        return extract_theses_from_text_chunked(text, filename, max_chars)\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following document and extract ALL thesis statements/arguments.\n",
    "\n",
    "A thesis is a claim, argument, or proposition that the document is making. Extract every distinct thesis statement.\n",
    "\n",
    "For each thesis, provide:\n",
    "1. The exact thesis statement (the claim being made)\n",
    "2. A brief 2-3 sentence description that accurately describes what the thesis is about\n",
    "\n",
    "Also identify relationships: if one thesis supports or provides evidence for another thesis, note that.\n",
    "\n",
    "IMPORTANT:\n",
    "- Analyze the ENTIRE document from beginning to end\n",
    "- Extract ALL theses, even if they seem similar (we will deduplicate later)\n",
    "- Be thorough - don't miss any arguments or claims\n",
    "- Each thesis should be a distinct statement\n",
    "- If theses are redundant or very similar, still list them (we'll handle deduplication)\n",
    "\n",
    "Return a JSON object with this structure:\n",
    "{{\n",
    "    \"theses\": [\n",
    "        {{\n",
    "            \"thesis\": \"The exact thesis statement\",\n",
    "            \"description\": \"2-3 sentences accurately describing what this thesis is about\"\n",
    "        }},\n",
    "        ...\n",
    "    ],\n",
    "    \"thesis_supports\": [\n",
    "        {{\n",
    "            \"source_thesis\": \"The supporting thesis statement\",\n",
    "            \"target_thesis\": \"The thesis statement it supports\"\n",
    "        }},\n",
    "        ...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Document text (FULL DOCUMENT - analyze everything):\n",
    "{text}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Analyzing document ({len(text):,} characters)...\")\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",  # or \"gpt-4o\" - both have 128K context\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert at analyzing documents and extracting thesis statements. Always return valid JSON. Be thorough and extract ALL thesis statements from the entire document, even if they seem similar.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=4096  # Maximum supported by gpt-4-turbo-preview\n",
    "        )\n",
    "        \n",
    "        result_text = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract JSON from the response\n",
    "        if \"```json\" in result_text:\n",
    "            result_text = result_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "        elif \"```\" in result_text:\n",
    "            result_text = result_text.split(\"```\")[1].split(\"```\")[0]\n",
    "        \n",
    "        # Find JSON object in the text\n",
    "        json_match = re.search(r'\\{.*\\}', result_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            result_text = json_match.group(0)\n",
    "        \n",
    "        result = json.loads(result_text.strip())\n",
    "        result[\"filename\"] = filename\n",
    "        return result\n",
    "    except json.JSONDecodeError as json_err:\n",
    "        print(f\"  âš  Failed to parse JSON: {json_err}\")\n",
    "        print(f\"  Debug: Response (first 1000 chars): {result_text[:1000] if 'result_text' in locals() else 'N/A'}\")\n",
    "        return {\"theses\": [], \"thesis_supports\": [], \"filename\": filename}\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"theses\": [], \"thesis_supports\": [], \"filename\": filename}\n",
    "\n",
    "def extract_theses_from_text_chunked(text: str, filename: str, chunk_size: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process very long documents in chunks with overlap to avoid splitting theses.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    overlap = 10000  # Overlap between chunks to avoid splitting theses\n",
    "    \n",
    "    # Split into chunks with overlap\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "        if i + chunk_size >= len(text):\n",
    "            break\n",
    "    \n",
    "    print(f\"    Processing {len(chunks)} chunks...\")\n",
    "    \n",
    "    all_theses = []\n",
    "    all_supports = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"    Chunk {i+1}/{len(chunks)} ({len(chunk):,} chars)...\")\n",
    "        chunk_result = extract_theses_from_text(chunk, f\"{filename}_chunk{i}\")\n",
    "        all_theses.extend(chunk_result.get(\"theses\", []))\n",
    "        all_supports.extend(chunk_result.get(\"thesis_supports\", []))\n",
    "    \n",
    "    return {\n",
    "        \"theses\": all_theses,\n",
    "        \"thesis_supports\": all_supports,\n",
    "        \"filename\": filename\n",
    "    }\n",
    "\n",
    "# Extract theses from all PDFs\n",
    "all_extractions = {}\n",
    "for filename, text in pdf_texts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Extracting theses from: {filename}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    extraction = extract_theses_from_text(text, filename)\n",
    "    all_extractions[filename] = extraction\n",
    "    print(f\"  âœ“ Found {len(extraction.get('theses', []))} theses, {len(extraction.get('thesis_supports', []))} support relationships\")\n",
    "\n",
    "print(f\"\\nâœ“ Extraction complete for {len(all_extractions)} document(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicating theses...\n",
      "âœ“ Found 32 unique theses (from 38 total)\n",
      "âœ“ Found 11 support relationships\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Deduplicate and number theses\n",
    "\n",
    "def deduplicate_theses(all_extractions: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine theses from all documents, deduplicate similar ones, and assign unique numbers.\n",
    "    \"\"\"\n",
    "    all_theses = []\n",
    "    thesis_to_number = {}\n",
    "    thesis_number = 1\n",
    "    \n",
    "    # First pass: collect all theses with their source files\n",
    "    for filename, extraction in all_extractions.items():\n",
    "        for thesis_data in extraction.get(\"theses\", []):\n",
    "            thesis = thesis_data.get(\"thesis\", \"\").strip()\n",
    "            description = thesis_data.get(\"description\", \"\").strip()\n",
    "            \n",
    "            if thesis:  # Only add non-empty theses\n",
    "                all_theses.append({\n",
    "                    \"thesis\": thesis,\n",
    "                    \"description\": description,\n",
    "                    \"source_file\": filename\n",
    "                })\n",
    "    \n",
    "    # Second pass: deduplicate similar theses\n",
    "    # Group theses by similarity (simple approach: exact match or very similar)\n",
    "    unique_theses = []\n",
    "    seen_theses = set()\n",
    "    \n",
    "    for thesis_data in all_theses:\n",
    "        thesis = thesis_data[\"thesis\"]\n",
    "        thesis_lower = thesis.lower().strip()\n",
    "        \n",
    "        # Check if we've seen a very similar thesis\n",
    "        is_duplicate = False\n",
    "        for seen in seen_theses:\n",
    "            # Simple similarity check: if one is contained in the other or very similar\n",
    "            if (thesis_lower in seen.lower() or seen.lower() in thesis_lower) and abs(len(thesis_lower) - len(seen)) < 20:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        \n",
    "        if not is_duplicate:\n",
    "            seen_theses.add(thesis)\n",
    "            unique_theses.append(thesis_data)\n",
    "            thesis_to_number[thesis] = thesis_number\n",
    "            thesis_number += 1\n",
    "    \n",
    "    # Build thesis supports relationships with numbers\n",
    "    thesis_supports = []\n",
    "    for filename, extraction in all_extractions.items():\n",
    "        for rel in extraction.get(\"thesis_supports\", []):\n",
    "            source = rel.get(\"source_thesis\", \"\").strip()\n",
    "            target = rel.get(\"target_thesis\", \"\").strip()\n",
    "            \n",
    "            # Find the unique thesis numbers\n",
    "            source_num = None\n",
    "            target_num = None\n",
    "            \n",
    "            for thesis, num in thesis_to_number.items():\n",
    "                if source and (source in thesis or thesis in source):\n",
    "                    source_num = num\n",
    "                if target and (target in thesis or thesis in target):\n",
    "                    target_num = num\n",
    "            \n",
    "            if source_num and target_num:\n",
    "                thesis_supports.append({\n",
    "                    \"source_thesis_number\": source_num,\n",
    "                    \"target_thesis_number\": target_num,\n",
    "                    \"source_thesis\": source,\n",
    "                    \"target_thesis\": target\n",
    "                })\n",
    "    \n",
    "    return {\n",
    "        \"theses\": unique_theses,\n",
    "        \"thesis_to_number\": thesis_to_number,\n",
    "        \"thesis_supports\": thesis_supports\n",
    "    }\n",
    "\n",
    "# Deduplicate and number\n",
    "print(\"Deduplicating theses...\")\n",
    "deduplicated = deduplicate_theses(all_extractions)\n",
    "print(f\"âœ“ Found {len(deduplicated['theses'])} unique theses (from {sum(len(e.get('theses', [])) for e in all_extractions.values())} total)\")\n",
    "print(f\"âœ“ Found {len(deduplicated['thesis_supports'])} support relationships\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Created review CSV: ..\\data\\analysis\\theses_for_review.csv\n",
      "\n",
      "CSV Summary:\n",
      "  Total theses: 32\n",
      "  Theses with support relationships: 9\n",
      "\n",
      "First few rows:\n",
      "   Thesis Number                                                                                                                                                            Thesis Statement                                                                                                                                                                                                                                                                                                                       Description Supports Thesis Numbers      Source File\n",
      "0              1                     Generative AI's true impact is still in its early stages, with second order applications and AI-native business models continuing to emerge and evolve.                                                                            This thesis posits that the full potential of generative AI has yet to be realized, suggesting that as the technology matures, new applications and business models specifically designed around AI capabilities will continue to surface and develop.                       2  AI 20250923.pdf\n",
      "1              2            Enterprises adopting verticalized approaches like SLMs, RAG pipelines, and vertical AI agents will build the most practical and defensible enterprise solutions.  The argument here is that businesses that tailor AI solutions to specific industry needs, through strategies like Service Lifecycle Management (SLMs), Rights and Governance (RAG) pipelines, and dedicated AI agents, will create solutions that are not only highly effective but also difficult for competitors to replicate.                          AI 20250923.pdf\n",
      "2              3                                                                                     Hybrid, human-in-the-loop AI systems will remain a core part of enterprise AI strategy.                                                                                                                    This thesis suggests that despite advances in AI, systems that incorporate both AI automation and human oversight will continue to be essential for managing complex and high-stakes tasks within enterprises.                       5  AI 20250923.pdf\n",
      "3              4                                                                        Inference demand is reshaping compute requirements, necessitating flexible, scalable infrastructure.                                                                                    The claim here is that as the use of AI models grows, the demand for real-time data processing (inference) is changing the landscape of computational infrastructure, requiring systems that can scale and adapt to varying performance needs.                       5  AI 20250923.pdf\n",
      "4              5                                                Agentic AI systems will require new infrastructure and cost-efficient frameworks to fully scale across enterprise workflows.                                                                                                              This thesis argues that for AI systems capable of autonomous decision-making (agentic AI) to be effectively integrated into enterprise operations, new, cost-effective infrastructural frameworks will be necessary.                          AI 20250923.pdf\n",
      "5              6  AI business models present unique investment challenges and opportunities compared to traditional SaaS, including higher risk but potential for exponential value capture.                                                        This statement contrasts AI and traditional software-as-a-service (SaaS) business models, highlighting the unique challenges and potential rewards of investing in AI, such as the unpredictable nature of AI outcomes versus the more consistent revenue streams of SaaS.                          AI 20250923.pdf\n",
      "6              7                AI companies using SaaS-style product-led growth playbooks are at high risk of trapping themselves in localized problems, preventing venture-backable scale.                                                                                            The argument here is that AI companies that apply growth strategies typical of SaaS businesses may limit their ability to scale by becoming too focused on narrow, immediate use cases instead of broader, more scalable applications.                      17  AI 20250923.pdf\n",
      "7              8                                             Argonautic's differentiated approach in managing its portfolio companies provides direct value across various stages of growth.                                                                                       This thesis claims that Argonautic's hands-on management style, which includes providing operational guidance, industry expertise, and strategic connections, significantly benefits its portfolio companies at both early and late stages.                          AI 20250923.pdf\n",
      "8              9                                                          Argonautic's high touch, versatile approach earns founder trust and gains excess allocation in fundraising rounds.                                                                                      The statement suggests that Argonautic's comprehensive and adaptable support for its portfolio companies not only builds strong relationships with founders but also positions Argonautic favorably in competitive fundraising environments.                          AI 20250923.pdf\n",
      "9             10                      Argonautic aggregates valuable information into its purpose-built flywheel, accelerating portfolio growth and steering founders to favorable outcomes.                                                                                                                     This thesis outlines how Argonautic's systematic process of gathering insights and interactions feeds into a cycle that enhances its portfolio companies' growth and guides them towards successful outcomes.                          AI 20250923.pdf\n",
      "\n",
      "ðŸ“ Next steps:\n",
      "  1. Open ..\\data\\analysis\\theses_for_review.csv in Excel or a text editor\n",
      "  2. Review each thesis statement and description\n",
      "  3. Ensure descriptions are 2-3 sentences and accurate\n",
      "  4. Verify and update 'Supports Thesis Numbers' column\n",
      "  5. Save as 'theses_reviewed.csv' when done\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create review-friendly CSV\n",
    "\n",
    "def create_review_csv(deduplicated: Dict[str, Any], output_file: Path):\n",
    "    \"\"\"\n",
    "    Create a CSV file optimized for human review.\n",
    "    \"\"\"\n",
    "    thesis_to_number = deduplicated[\"thesis_to_number\"]\n",
    "    thesis_supports = deduplicated[\"thesis_supports\"]\n",
    "    \n",
    "    # Build a map of which theses each thesis supports\n",
    "    supports_map = {}\n",
    "    for rel in thesis_supports:\n",
    "        source_num = rel[\"source_thesis_number\"]\n",
    "        target_num = rel[\"target_thesis_number\"]\n",
    "        \n",
    "        if source_num not in supports_map:\n",
    "            supports_map[source_num] = []\n",
    "        supports_map[source_num].append(target_num)\n",
    "    \n",
    "    # Create rows for CSV\n",
    "    rows = []\n",
    "    for thesis_data in deduplicated[\"theses\"]:\n",
    "        thesis = thesis_data[\"thesis\"]\n",
    "        thesis_num = thesis_to_number[thesis]\n",
    "        \n",
    "        # Get list of thesis numbers this thesis supports\n",
    "        supports_list = sorted(supports_map.get(thesis_num, []))\n",
    "        supports_str = \", \".join(map(str, supports_list)) if supports_list else \"\"\n",
    "        \n",
    "        rows.append({\n",
    "            \"Thesis Number\": thesis_num,\n",
    "            \"Thesis Statement\": thesis,\n",
    "            \"Description\": thesis_data[\"description\"],\n",
    "            \"Supports Thesis Numbers\": supports_str,\n",
    "            \"Source File\": thesis_data[\"source_file\"]\n",
    "        })\n",
    "    \n",
    "    # Sort by thesis number\n",
    "    rows.sort(key=lambda x: x[\"Thesis Number\"])\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the review CSV\n",
    "output_file = OUTPUT_FOLDER / \"theses_for_review.csv\"\n",
    "df = create_review_csv(deduplicated, output_file)\n",
    "\n",
    "print(f\"\\nâœ“ Created review CSV: {output_file}\")\n",
    "print(f\"\\nCSV Summary:\")\n",
    "print(f\"  Total theses: {len(df)}\")\n",
    "print(f\"  Theses with support relationships: {len(df[df['Supports Thesis Numbers'] != ''])}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head(10).to_string())\n",
    "print(f\"\\nðŸ“ Next steps:\")\n",
    "print(f\"  1. Open {output_file} in Excel or a text editor\")\n",
    "print(f\"  2. Review each thesis statement and description\")\n",
    "print(f\"  3. Ensure descriptions are 2-3 sentences and accurate\")\n",
    "print(f\"  4. Verify and update 'Supports Thesis Numbers' column\")\n",
    "print(f\"  5. Save as 'theses_reviewed.csv' when done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
